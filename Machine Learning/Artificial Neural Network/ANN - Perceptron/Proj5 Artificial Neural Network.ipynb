{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 : Artificial Neural Network\n",
    "* CS 5300 : Artificial Intelligent\n",
    "* Kyu Cho\n",
    "* 5/8/16\n",
    "\n",
    "## Data Description\n",
    "### Intro\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "### Variables\n",
    "Note: This data already has been preprocessed.\n",
    "\n",
    "\n",
    "**Sex** : Sex (1 = Female; 0 = Male)  \n",
    "**FamilySize** : Family (1 = Aboard with Family; 0 = Aboard without Family)  \n",
    "**Child** : Child (age < 16) (1 = Child; 0 = Not Child)  \n",
    "**Pclass.2** : Passenger class (1 = 2nd Class; 0 = 1st or 3rd Class)  \n",
    "**Pclass.3** : Passenger class (1 = 3rd Class; 0 = 1st or 2nd Class)  \n",
    "Note : It does not have value for 1st class because if Pclass.2 = 0 and Pclass.3 = 0 meaning it's Pclass.1.  \n",
    "**Survive** : Survival (0 = No; 1 = Yes)  \n",
    "\n",
    "## Implementation\n",
    "Here is the outline of this program.\n",
    " * Convert an Datafrane into a NumPy array.\n",
    " * Implement the link function for ANN.\n",
    " * Write a function to compute the derivative of the log likelihood function with respect to a single coefficient.\n",
    " * Implement gradient ascent.\n",
    " * Given a set of coefficients, predict survive.\n",
    " * Compute classification accuracy for the ANN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  FamilySize  Child  Pclass.2  Pclass.3  Survived\n",
      "0    0           0      0         0         1         0\n",
      "1    1           0      0         0         0         1\n",
      "2    1           0      0         0         0         1\n",
      "3    0           0      0         0         1         0\n",
      "4    0           0      0         0         0         0\n",
      "('# of Alives =', 278)\n",
      "('# of Deads =', 434)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "os.chdir('C:\\Users\\Kyu\\Documents\\python\\data')\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(train.head())\n",
    "\n",
    "features = ['Sex',\n",
    "            'FamilySize',\n",
    "            'Child',\n",
    "            'Pclass.2',\n",
    "            'Pclass.3']\n",
    "target = 'Survived'\n",
    "train[train.Survived == 0] = -1\n",
    "test[test.Survived == 0] = -1\n",
    "print ('# of Alives =', len(train[train['Survived'] == 1]))\n",
    "print ('# of Deads =', len(train[train['Survived'] == -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dataframe into NumPy array\n",
    "\n",
    "Two arrays are returned: one representing features and another representing class labels. \n",
    "Note that the feature matrix includes an additional column 'intercept' to take account of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features # Adding intercept variables in to the features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.as_matrix()\n",
    "    label_sarray = data_sframe[label]\n",
    "    label_array = label_sarray.as_matrix()\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the data into NumPy arrays\n",
    "feature_matrix, survived = get_numpy_data(train, features, 'Survived') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute conditional probability with link function\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ represents the value of **each feature**  $\\mathbf{x}_i$. Complete the following function that implements the link function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    dot_product = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = 1/(1+np.exp(-dot_product))\n",
    "\n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. How the link function works with matrix algebra\n",
    "\n",
    "Since the each features are stored as columns in **feature_matrix**, each $i$-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$:\n",
    "$$\n",
    "[\\text{feature_matrix}] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\\n",
    "h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^T h(\\mathbf{x}_i)$ is obtained by multiplying **feature_matrix** and the coefficient vector $\\mathbf{w}$.\n",
    "$$\n",
    "[\\text{score}] =\n",
    "[\\text{feature_matrix}]\\mathbf{w} =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\mathbf{w}\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T\\mathbf{w} \\\\\n",
    "h(\\mathbf{x}_2)^T\\mathbf{w} \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\\mathbf{w}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}^T h(\\mathbf{x}_1) \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute derivative of log likelihood with respect to a single coefficient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "We will now write a function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n",
    "* `errors` vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n",
    "* `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood simplifies the derivation of the gradient and is more numerically stable.  Due to its numerical stability, we will use the log likelihood instead of the likelihood to assess the algorithm.\n",
    "\n",
    "The log likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "We provide a function to compute the log likelihood for the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, survived, coefficients):\n",
    "    indicator = (survived==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    logexp = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    # Simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - logexp)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradiant decent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def ann(feature_matrix, survived, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (survived==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        new_coefficients = np.ndarray(shape=coefficients.shape)\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            derivative = feature_derivative(errors, feature_matrix[:, j])\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            new_coefficients[j] = coefficients[j] + step_size * derivative\n",
    "        \n",
    "        coefficients = new_coefficients\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, survived, coefficients)\n",
    "            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                 (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -493.48475371\n",
      "iteration   1: log likelihood of observed labels = -493.44871947\n",
      "iteration   2: log likelihood of observed labels = -493.41268984\n",
      "iteration   3: log likelihood of observed labels = -493.37666482\n",
      "iteration   4: log likelihood of observed labels = -493.34064441\n",
      "iteration   5: log likelihood of observed labels = -493.30462860\n",
      "iteration   6: log likelihood of observed labels = -493.26861740\n",
      "iteration   7: log likelihood of observed labels = -493.23261081\n",
      "iteration   8: log likelihood of observed labels = -493.19660882\n",
      "iteration   9: log likelihood of observed labels = -493.16061143\n",
      "iteration  10: log likelihood of observed labels = -493.12461865\n",
      "iteration  11: log likelihood of observed labels = -493.08863047\n",
      "iteration  12: log likelihood of observed labels = -493.05264690\n",
      "iteration  13: log likelihood of observed labels = -493.01666792\n",
      "iteration  14: log likelihood of observed labels = -492.98069355\n",
      "iteration  15: log likelihood of observed labels = -492.94472378\n",
      "iteration  20: log likelihood of observed labels = -492.76494391\n",
      "iteration  30: log likelihood of observed labels = -492.40572889\n",
      "iteration  40: log likelihood of observed labels = -492.04697296\n",
      "iteration  50: log likelihood of observed labels = -491.68867552\n",
      "iteration  60: log likelihood of observed labels = -491.33083595\n",
      "iteration  70: log likelihood of observed labels = -490.97345365\n",
      "iteration  80: log likelihood of observed labels = -490.61652799\n",
      "iteration  90: log likelihood of observed labels = -490.26005837\n",
      "iteration 100: log likelihood of observed labels = -489.90404418\n",
      "iteration 200: log likelihood of observed labels = -486.36881677\n",
      "iteration 300: log likelihood of observed labels = -482.87846355\n",
      "iteration 400: log likelihood of observed labels = -479.43238094\n",
      "iteration 500: log likelihood of observed labels = -476.02996988\n",
      "iteration 600: log likelihood of observed labels = -472.67063597\n",
      "iteration 700: log likelihood of observed labels = -469.35378968\n",
      "iteration 800: log likelihood of observed labels = -466.07884647\n"
     ]
    }
   ],
   "source": [
    "coefficients = ann(feature_matrix, survived, initial_coefficients=np.zeros(6),\n",
    "                                   step_size=1e-7, max_iter=801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class predictions for a data point $\\mathbf{x}$ can be computed from the coefficients $\\mathbf{w}$ using the following formula:\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{x}_i^T\\mathbf{w} > 0 \\\\\n",
    "      -1 & \\mathbf{x}_i^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Now, we will write some code to compute class predictions. We will do this in two steps:\n",
    "* **Step 1**: First compute the **scores** using **feature_matrix** and **coefficients** using a dot product.\n",
    "* **Step 2**: Using the formula above, compute the class predictions from the scores.\n",
    "\n",
    "Step 1 can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01830103,  0.01830103, -0.10945666, -0.10945666, -0.10945666,\n",
       "        0.05911417,  0.05692958,  0.07802963,  0.01830103])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "scores = np.dot(feature_matrix, coefficients)\n",
    "scores[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2\n",
    "npa_possent = (scores > 0)\n",
    "npa_possent = npa_possent[npa_possent == True]\n",
    "npa_possent[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "('# Correctly classified   =', 679)\n",
      "('# Incorrectly classified =', 33)\n",
      "('# Total                  =', 712)\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.95\n"
     ]
    }
   ],
   "source": [
    "predictions = np.empty_like(scores)\n",
    "for i in range(len(scores)):\n",
    "    predictions[i] = +1 if scores[i]> 0 else -1\n",
    "\n",
    "measures = predictions == train['Survived'] \n",
    "num_mistakes = len(measures[measures == False])\n",
    "accuracy = len(measures[measures == True]) / float(len(measures)) \n",
    "print(\"-----------------------------------------------------\")\n",
    "print('# Correctly classified   =', len(train) - num_mistakes)\n",
    "print('# Incorrectly classified =', num_mistakes)\n",
    "print('# Total                  =', len(train))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print('Accuracy = %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
